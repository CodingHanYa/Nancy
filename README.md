# 一个采用C++11编写轻量级Linux网络库

Nancy是采用c++11编写的Linux轻量级网络库，风格上**偏Linux原生**，抽象较少。主要封装了**网络模块**和**日志模块**，其余提供了如定时器、内存池、socket封装等实用的组件。目前主要提供对TCP的支持，部分组件同样支持UDP。

## 模块与功能列表

### Net：对epoll、socket、signal等的封装

- reactor ： 基于Epoll的Linux反应堆，采用事件回调的方式简化网络编程接口，可以自由选择epoll的ET/LT模式。支持**超时处理**和**信号事件**，
- creactors(concurrent reactors)：多节点并发反应堆，基于reactor和socket实现，采用**one loop per thread**模型，为多核系统提供更高的并发能力。（测试结果见下）。接口方面可以为不同的**工作节点**定制回调，也可设置统一回调。支持epoll的ET模式和LT模式。
- socket：封装服务端和客户端Linux socket，以及Unix本地通信socketpair等。
- fd： 封装了Linux常用的文件描述符操作如设置设置内核缓冲区大小、设置非阻塞、设置nondelay等待。

### Logger：轻量级异步日志系统

- 线程安全：支持**多生产者单消费者模型**进行异步记录日志。
- 低延迟：采用**压缩技术**对用户层的流进行**压缩存储**，减少同步数据到异步线程的开销。
- 低碎片化：通过**栈内存+大缓冲**来避免在堆上申请小块内存，减少长时间运行时产生的**内存碎片**。
- 日志安全：支持**\[info]、[warn]、[critical]**三种级别的日志记录，其中[critical]级别确保日志被及时写入硬盘中。
- 滚动日志：支持滚动日志，能够根据用户设定的字节数自动分割文件。

### 其它组件

- Timer： 基于红黑树的轻量级定时器

	- 高效：在log(n)时间复杂度下的查询及修改性能（具体表现见下）

	- 低内存：采用红黑树，在内存方面相比哈希表等额外开销更少

- Memorys: 基于哈希表的轻量级内存池

	- 高速存取：采用哈希表存储内存单元，在O1复杂度下实现增删查的功能。


## demo

### echo-server

```C++
#include "nancy/net/reactor.h"
using namespace nc;

int main() {
    net::reactor rec;
    net::tcp_serv_socket sock;  
    sock.listen_req(net::localhost, 9090);  // 监听IP+端口
    rec.add_socket(sock.get_fd(), net::event::readable, net::pattern::lt, [&sock, &rec](int){
        int tmp = sock.accept_req();  
        net::set_nonblocking(tmp);   // 非阻塞+lt实现
        rec.add_socket(tmp, net::event::readable, net::pattern::lt);
    });
    char buf[1024];
    rec.set_readable_cb([&buf](int fd) {
        int rcv = read(fd, buf, 1024);
        write(fd, buf, rcv);
    });
    rec.activate();  // 激活
}
```
可以看到事件驱动的回调让事情变得简单。而基于reactor开发的网络通讯模型，能省去许多面向Linux接口的烦恼，让我们在20行内在完成简单的回声服务功能的同时又能设置非阻塞fd+定制epoll的触发模式，提高性能。





## Benchmark

测试环境：

- 编译器：g++ 9.4.0 ，开启O3优化
- 系统：ubuntu20.04  
- 处理器：AMD Ryzen 7 5800H with Radeon Graphics     3.20 GHz
- CPU：16核

### 网络模块

#### pingpong测试

测试背景：

- 我们需要看看Nancy在面对并发数逐渐上升的情况下reactor和creactors的最大吞吐量的情况。

实现原理：

- 服务端：在单线程测试中我们在服务端采用reactor监听端口，当客户端发送数据时我们不断读取16k数据。而在多线程测试中我们采用的是creactors（concurrent reactors），同样每次只读取16k的数据。
- 客户端：无论在单线程测试还是在多线程测试中我们都采用one thread one reactor来发送数据，单次只发送16k数据。

源码位置： `/benchmark/pingpong`

指标说明：1v1：代表服务端线程数vs客户端线程数，以下同理。结果单位：**Mb/s**

|   并发连接数/线程数(server vs client)   |   1v1   |   server-cpu   |   client-cpu   |
| :-------------------------------------: | :-----: | :------------: | :------------: |
|                   20                    | 8691.65 |     97.30%     |      100%      |
|                   50                    | 7701.1  |     98.70%     |      100%      |
|                   100                   | 5611.88 |     83.00%     |      100%      |
|                   200                   | 5381.74 |     81.00%     |      100%      |
|                   400                   | 4904.17 |     80.30%     |    100.00%     |
|                   800                   | 4203.22 |     77.00%     |      100%      |
|                                         |         |                |                |
| **并发连接数/线程数(server vs client)** | **2v2** | **server-cpu** | **client-cpu** |
|                   20                    | 8817.59 |    183.00%     |      200%      |
|                   50                    | 8991.26 |    183.10%     |      200%      |
|                   100                   | 8093.26 |    164.10%     |      200%      |
|                   200                   | 7597.44 |    172.70%     |      200%      |
|                   400                   | 7040.18 |    162.30%     |      200%      |
|                   800                   | 6091.03 |    142.00%     |      200%      |
|                                         |         |                |                |
| **并发连接数/线程数(server vs client)** | **2v4** | **server-cpu** | **client-cpu** |
|                   20                    | 6153.78 |    200.70%     |      186%      |
|                   50                    | 5902.2  |    200.00%     |    179.30%     |
|                   100                   | 5874.32 |    199.70%     |    227.30%     |
|                   200                   | 5882.35 |      200%      |    231.30%     |
|                   400                   | 5801.11 |    200.00%     |    258.30%     |
|                   800                   | 5562.88 |    200.70%     |    274.40%     |
|                                         |         |                |                |
| **并发连接数/线程数(server vs client)** | **4v4** | **server-cpu** | **client-cpu** |
|                   20                    | 7369.28 |      400%      |      322%      |
|                   50                    | 8850.33 |    349.70%     |    400.00%     |
|                   100                   | 8583.45 |    331.90%     |    400.00%     |
|                   200                   | 7257.27 |    342.50%     |    400.00%     |
|                   400                   | 7076.89 |    253.70%     |    400.00%     |
|                   800                   | 6788.89 |    217.00%     |    400.00%     |

结果说明：

- 在单线程且并发数较少的情况下，reactor的最高吞吐量可以做到**8000Mb/s~9000Mb/s**左右，表现良好。但是当并发数逐渐上升之后，单线程reactor的吞吐量就会不断下降。具体原因可能是本机的**单线程IO性能**已经触及本机的瓶颈了，更多的并发只会导致内核需要处理更多的事件，每次需要把更多可读连接通过链表保存起来，再通过epoll_wait进行通知，这些开销反而降低了总体的吞吐性能。
- 面对前面这种情况，我们采用并发反应堆可以更好的应对。如上，当server的线程数和client的线程数相同时(2v2)，其吞吐量普遍要高于单线程reactor。因为不论是server还是client，分摊到各个epoll的连接都变少了，负面影响都更小。

最后：

​	以上结果只说明Nancy在编写reactor模块时没有犯什么大错误，不能说明Nancy的吞吐量有何过人之处。因为其实对epoll进行封装的写法是相对固定的，无法在编码上做太多改进。作为网络库的一个组件来说，与其它组件协同的难易程度，其接口的安全性、易用性等也许更值得考虑。同时，限制服务端性能的往往不是“响应”，而是IO与之后的处理过程。



### 日志模块

#### 并发延迟百分比测试

- 测试背景：
	- 在**异步数据争用**和**数据压缩**方面做了处理之后，我们需要知道目前日志系统的**延迟情况**。
- 实现原理：
	- 不同日志系统都开启一条异步消费线程，创建多条（1-4）生产者线程不断往异步线程发送日志记录，并记录每条日志发送延迟。
	- 我们会记录每条线程的结果并并将线程结果进行平均处理，得到最终结果。

| 线程数 | 日志系统 | 50%  | 75%  | 90%  | 99.00% | 99.90% | 最差 | 平均值  |
| ------ | -------- | ---- | ---- | ---- | ------ | ------ | ---- | ------- |
| 1      | nclog    | 0    | 0    | 1    | 8      | 11     | 100  | 0.2579  |
| 2      | nclog    | 0    | 0    | 1    | 5      | 13     | 106  | 0.2825  |
| 3      | nclog    | 0    | 0    | 1    | 9      | 13     | 56   | 0.3848  |
| 4      | nclog    | 0    | 0    | 1    | 9      | 18     | 618  | 0.5952  |
| 1      | spdlog   | 0    | 1    | 1    | 1      | 14     | 50   | 0.2956  |
| 2      | spdlog   | 0    | 1    | 30   | 45     | 91     | 217  | 4.3377  |
| 3      | spdlog   | 42   | 47   | 56   | 118    | 184    | 409  | 42.9147 |
| 4      | spdlog   | 51   | 56   | 70   | 146    | 229    | 4967 | 52.6044 |

结果说明：可以看到Nancy的日志系统nclog在总体的延迟水平远低于spdlog，且线程数越多，nclog的表现和spdlog的表现相差越大。



## TODO

- 增加更多的**FD**设置接口封装
- 增加对**UDP**的支持
- 增加更多**用例**
- 考虑从项目中分离出日志模块

